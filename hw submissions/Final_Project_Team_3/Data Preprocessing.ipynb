{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<left>Correlation One DS4A/W </left>\n",
    "<br>\n",
    "<h3>Team - 3</h3>\n",
    "<h3>TA - Oretha Domfeh</h3>\n",
    "\n",
    "<h2><center> Data Preprocessing </center></h2>\n",
    "\n",
    "<center> Amber Lee </center>\n",
    "<center> Cindy Zhang </center>\n",
    "<center> Hejia Zhang </center>\n",
    "<center> Wafer Hsu </center>\n",
    "<br>\n",
    "\n",
    "<center>Monday, Jul 24</center>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.9/site-packages (4.11.1)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.3.1)\r\n",
      "Collecting newspaper3k\r\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m211.1/211.1 kB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (4.11.1)\r\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (9.4.0)\r\n",
      "Requirement already satisfied: PyYAML>=3.11 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (6.0)\r\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (1.1.0)\r\n",
      "Requirement already satisfied: lxml>=3.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (4.9.1)\r\n",
      "Requirement already satisfied: nltk>=3.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (3.7)\r\n",
      "Requirement already satisfied: requests>=2.10.0 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (2.28.1)\r\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\r\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m81.1/81.1 kB\u001B[0m \u001B[31m1.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: tldextract>=2.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (3.2.0)\r\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\r\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting jieba3k>=0.35.1 (from newspaper3k)\r\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.4/7.4 MB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (2.8.2)\r\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\r\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.3.1)\r\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.9/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\r\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\r\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: click in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.2.1->newspaper3k) (8.0.4)\r\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.2.1->newspaper3k) (1.1.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.2.1->newspaper3k) (2022.7.9)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.2.1->newspaper3k) (4.64.1)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->newspaper3k) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->newspaper3k) (1.26.11)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->newspaper3k) (2022.9.24)\r\n",
      "Requirement already satisfied: requests-file>=1.4 in /opt/anaconda3/lib/python3.9/site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\r\n",
      "Requirement already satisfied: filelock>=3.0.8 in /opt/anaconda3/lib/python3.9/site-packages (from tldextract>=2.0.1->newspaper3k) (3.6.0)\r\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\r\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13541 sha256=c0edc9ad0364606115b836e13f899baa201d1ab82a8fcf995bbad4c67631f720\r\n",
      "  Stored in directory: /Users/cindy/Library/Caches/pip/wheels/94/ad/df/a2a01300cea47d5695f242f7e925a805970106fd9e4b151468\r\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=cf03bf4817bc84e648544148198e2ef4dc7a066d6021fb7db3451df0e4ba431b\r\n",
      "  Stored in directory: /Users/cindy/Library/Caches/pip/wheels/43/4a/c2/61a371b2524ac90805391c660d8dc4505705297f25e2b85a5d\r\n",
      "  Building wheel for jieba3k (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398382 sha256=36692c41d7199eebf6c708d9c75195236769cd38fdd6a47bae402f916f07b213\r\n",
      "  Stored in directory: /Users/cindy/Library/Caches/pip/wheels/c2/22/59/8214a8d6357e9f540ce1f37f9a4362b6156b4ca81b37f1945f\r\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=20e165eb18b63f1f8ac946911ee1ebf85e6484ecdb333ec81c29c388c48c1d2c\r\n",
      "  Stored in directory: /Users/cindy/Library/Caches/pip/wheels/65/7a/a7/78c287f64e401255dff4c13fdbc672fed5efbfd21c530114e1\r\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\r\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, feedfinder2, newspaper3k\r\n",
      "Successfully installed feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 sgmllib3k-1.0.0 tinysegmenter-0.3\r\n",
      "Collecting git+https://github.com/ranahaani/GNews.git\r\n",
      "  Cloning https://github.com/ranahaani/GNews.git to /private/var/folders/97/ry8p7x356bbbk7qdw9k14pyr0000gn/T/pip-req-build-l10ne1ck\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ranahaani/GNews.git /private/var/folders/97/ry8p7x356bbbk7qdw9k14pyr0000gn/T/pip-req-build-l10ne1ck\r\n",
      "  Resolved https://github.com/ranahaani/GNews.git to commit 8591313e3fdaaf44e2e09f2265254fc3aaea8b56\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: feedparser~=6.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from gnews==0.2.7) (6.0.10)\r\n",
      "Collecting bs4~=0.0.1 (from gnews==0.2.7)\r\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting beautifulsoup4~=4.9.3 (from gnews==0.2.7)\r\n",
      "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m115.8/115.8 kB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting pymongo~=3.12.0 (from gnews==0.2.7)\r\n",
      "  Downloading pymongo-3.12.3-cp39-cp39-macosx_10_9_x86_64.whl (395 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m395.4/395.4 kB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting dnspython~=1.16.0 (from gnews==0.2.7)\r\n",
      "  Downloading dnspython-1.16.0-py2.py3-none-any.whl (188 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m188.4/188.4 kB\u001B[0m \u001B[31m603.4 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting python-dotenv~=0.19.0 (from gnews==0.2.7)\r\n",
      "  Downloading python_dotenv-0.19.2-py2.py3-none-any.whl (17 kB)\r\n",
      "Collecting requests==2.26.0 (from gnews==0.2.7)\r\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.3/62.3 kB\u001B[0m \u001B[31m1.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests==2.26.0->gnews==0.2.7) (1.26.11)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests==2.26.0->gnews==0.2.7) (2022.9.24)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests==2.26.0->gnews==0.2.7) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests==2.26.0->gnews==0.2.7) (3.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4~=4.9.3->gnews==0.2.7) (2.3.1)\r\n",
      "Requirement already satisfied: sgmllib3k in /opt/anaconda3/lib/python3.9/site-packages (from feedparser~=6.0.2->gnews==0.2.7) (1.0.0)\r\n",
      "Building wheels for collected packages: gnews, bs4\r\n",
      "  Building wheel for gnews (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for gnews: filename=gnews-0.2.7-py3-none-any.whl size=16161 sha256=c96a5b0cc03d40a49b997e2361ce877b195278d5b611c5958355f2cbd8c5ae9e\r\n",
      "  Stored in directory: /private/var/folders/97/ry8p7x356bbbk7qdw9k14pyr0000gn/T/pip-ephem-wheel-cache-sfptsfot/wheels/17/57/85/1c4d5a4488fc09b600274e90a6a5ea4f8f63d672688a9ac885\r\n",
      "  Building wheel for bs4 (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=61f568861651c23b3556c58ace7344dd3e23f08f644c1d017511af9423833ff4\r\n",
      "  Stored in directory: /Users/cindy/Library/Caches/pip/wheels/73/2b/cb/099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\r\n",
      "Successfully built gnews bs4\r\n",
      "Installing collected packages: requests, python-dotenv, pymongo, dnspython, beautifulsoup4, bs4, gnews\r\n",
      "  Attempting uninstall: requests\r\n",
      "    Found existing installation: requests 2.28.1\r\n",
      "    Uninstalling requests-2.28.1:\r\n",
      "      Successfully uninstalled requests-2.28.1\r\n",
      "  Attempting uninstall: python-dotenv\r\n",
      "    Found existing installation: python-dotenv 1.0.0\r\n",
      "    Uninstalling python-dotenv-1.0.0:\r\n",
      "      Successfully uninstalled python-dotenv-1.0.0\r\n",
      "  Attempting uninstall: beautifulsoup4\r\n",
      "    Found existing installation: beautifulsoup4 4.11.1\r\n",
      "    Uninstalling beautifulsoup4-4.11.1:\r\n",
      "      Successfully uninstalled beautifulsoup4-4.11.1\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\r\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\r\n",
      "conda-repo-cli 1.0.20 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\r\n",
      "conda-repo-cli 1.0.20 requires nbformat==5.4.0, but you have nbformat 5.5.0 which is incompatible.\r\n",
      "conda-repo-cli 1.0.20 requires requests==2.28.1, but you have requests 2.26.0 which is incompatible.\r\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 2.0.1 which is incompatible.\r\n",
      "yfinance 0.2.12 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.9.3 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed beautifulsoup4-4.9.3 bs4-0.0.1 dnspython-1.16.0 gnews-0.2.7 pymongo-3.12.3 python-dotenv-0.19.2 requests-2.26.0\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install beautifulsoup4\n",
    "!{sys.executable} -m pip install newspaper3k\n",
    "!{sys.executable} -m pip install git+https://github.com/ranahaani/GNews.git"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/cindy/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from gnews import GNews\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Quandl Python Tables API Function\n",
    "def grab_quandl_table(\n",
    "    table_path,\n",
    "    avoid_download=False,\n",
    "    replace_existing=False,\n",
    "    date_override=None,\n",
    "    allow_old_file=False,\n",
    "    **kwargs,\n",
    "):\n",
    "    root_data_dir = os.path.join(os.getcwd(), \"quandl_data_table_downloads\")\n",
    "    data_symlink = os.path.join(root_data_dir, f\"{table_path}_latest.zip\")\n",
    "    if avoid_download and os.path.exists(data_symlink):\n",
    "        print(f\"Skipping any possible download of {table_path}\")\n",
    "        return data_symlink\n",
    "\n",
    "    table_dir = os.path.dirname(data_symlink)\n",
    "    if not os.path.isdir(table_dir):\n",
    "        print(f'Creating new data dir {table_dir}')\n",
    "        os.makedirs(table_dir)\n",
    "\n",
    "    if date_override is None:\n",
    "        my_date = dt.datetime.now().strftime(\"%Y%m%d\")\n",
    "    else:\n",
    "        my_date = date_override\n",
    "    data_file = os.path.join(root_data_dir, f\"{table_path}_{my_date}.zip\")\n",
    "\n",
    "    if os.path.exists(data_file):\n",
    "        file_size = os.stat(data_file).st_size\n",
    "        if replace_existing or not file_size > 0:\n",
    "            print(f\"Removing old file {data_file} size {file_size}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Data file {data_file} size {file_size} exists already, no need to download\"\n",
    "            )\n",
    "            return data_file\n",
    "\n",
    "    api_key = os.environ.get('Cindy_Quandl_API')\n",
    "    dl = quandl.export_table(\n",
    "        table_path, filename=data_file, api_key=api_key, **kwargs\n",
    "    )\n",
    "    file_size = os.stat(data_file).st_size\n",
    "    if os.path.exists(data_file) and file_size > 0:\n",
    "        print(f\"Download finished: {file_size} bytes\")\n",
    "        if not date_override:\n",
    "            if os.path.exists(data_symlink):\n",
    "                print(f\"Removing old symlink\")\n",
    "                os.unlink(data_symlink)\n",
    "            print(f\"Creating symlink: {data_file} -> {data_symlink}\")\n",
    "            os.symlink(\n",
    "                data_file, data_symlink,\n",
    "            )\n",
    "    else:\n",
    "        print(f\"Data file {data_file} failed download\")\n",
    "        return\n",
    "    return data_symlink if (date_override is None or allow_old_file) else \"NoFileAvailable\"\n",
    "\n",
    "def fetch_quandl_table(table_path, avoid_download=True, **kwargs):\n",
    "    return pd.read_csv(\n",
    "        grab_quandl_table(table_path, avoid_download=avoid_download, **kwargs)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Retrieving Stock Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Set up"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Universe\n",
    "- S&P 500 stocks that are updated recently\n",
    "- At least 5 years of price data (2018-01-01 to 2023-05-31), IPO after 2018-01-01 are excluded\n",
    "- Have at least 100 trading days of price data before 2018-01-01 for rolling window Fama-French exposure calculation (IPO before 2017-08-16)\n",
    "- Because of the lacking data of Fama-French 5 factors returns, the universe is updated to 2018-01-01 to 2023-04-28"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Full market stock data\n",
    " full_price = fetch_quandl_table('QUOTEMEDIA/PRICES', avoid_download=False)\n",
    "# S&P 500 tickers list\n",
    "SP500 = pd.read_excel(\"S&P 500 tickers.xlsx\")\n",
    "# Fama-French 5 factors returns\n",
    "F_F_5_Factors = pd.read_csv(\"F_F_Research_Data_5_Factors_daily.csv\", skiprows = 3)\n",
    "F_F_5_Factors.columns = ['Date', 'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']\n",
    "F_F_5_Factors['Date'] = pd.to_datetime(F_F_5_Factors['Date'], format='%Y%m%d')\n",
    "F_F_5_Factors.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Only keep the stocks that are in the S&P 500\n",
    "universe = full_price[full_price['ticker'].isin(SP500['Symbol'])].copy().sort_values(by = ['ticker', 'date'])\n",
    "universe['date'] = pd.to_datetime(universe['date'])\n",
    "# Only keep the stocks that are in the S&P 500\n",
    "universe = full_price[full_price['ticker'].isin(SP500['Symbol'])].copy().sort_values(by = ['ticker', 'date'])\n",
    "universe['date'] = pd.to_datetime(universe['date'])\n",
    "universe.drop(universe[(universe.ticker.isin(SP500_start_date[SP500_start_date.start_date > '2017-08-16'].ticker))].index, inplace = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set up the universe time series precisely\n",
    "universe = universe[(universe['date'] >= '2017-08-15') & (universe['date'] < '2023-06-01')].sort_values(by = ['ticker', 'date'])[['ticker', 'date', 'adj_close', 'adj_volume']].reset_index(drop = True)\n",
    "# Calculate the daily return for each ticker\n",
    "universe['return'] = universe.groupby('ticker')['adj_close'].pct_change()\n",
    "universe = universe.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 F-F 5 factors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- The Fama-French 5 factors returns are updated daily.\n",
    "- The Fama-French 5 factors returns are risk premiums of the market which are not directly comparable to the returns of the stocks.\n",
    "- We will run the regression of daily excess returns of each stock on the Fama-French 5 factors to get the beta values as that stock's exposures to each risk premium in the model.\n",
    "- To prevent look ahead bias, I will use the expected return of the previous 100 days as the exposure to each risk premium of th next day. In other words, we will run 100 trading days rolling window regression of daily excess returns against Fama-French 5 factors returns.\n",
    "- Since the most recent factor return data that is available is 2023-04-28, our universe is updated to 2018-01-01 to 2023-04-28."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Match the date of Fama-French 5 factors to the universe date time series\n",
    "F_F_5_Factors = F_F_5_Factors[(F_F_5_Factors['Date'] >= '2017-08-15') & (F_F_5_Factors['Date'] < '2023-06-01')].reset_index(drop = True)\n",
    "F_F_regression = universe.merge(F_F_5_Factors, left_on = 'date', right_on = 'Date', how = 'left')\n",
    "F_F_regression['excess_return'] = F_F_regression['return'] - F_F_regression['RF']\n",
    "# The time range of the universe shrinks because of the lacking information of Fama-French 5 factor returns data\n",
    "F_F_regression = F_F_regression.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run 100 trading days rolling window regression of daily excess returns against Fama-French 5 factors returns\n",
    "F_F_regression['Mkt-RF_beta'] = np.nan\n",
    "F_F_regression['SMB_beta'] = np.nan\n",
    "F_F_regression['HML_beta'] = np.nan\n",
    "F_F_regression['RMW_beta'] = np.nan\n",
    "F_F_regression['CMA_beta'] = np.nan\n",
    "for ticker, group in F_F_regression.groupby('ticker'):\n",
    "    for idx in range(100, len(group)):\n",
    "        X = group.loc[group.index[idx] - 100:group.index[idx] - 1, ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']]\n",
    "        y = group.loc[group.index[idx] - 100:group.index[idx] - 1, 'excess_return']\n",
    "        #print(group.loc[group.index[idx]])\n",
    "        coefficients = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "        F_F_regression.loc[group.index[idx], 'Mkt-RF_beta'] = coefficients[0]\n",
    "        F_F_regression.loc[group.index[idx], 'SMB_beta'] = coefficients[1]\n",
    "        F_F_regression.loc[group.index[idx], 'HML_beta'] = coefficients[2]\n",
    "        F_F_regression.loc[group.index[idx], 'RMW_beta'] = coefficients[3]\n",
    "        F_F_regression.loc[group.index[idx], 'CMA_beta'] = coefficients[4]\n",
    "        #print(group.loc[group.index[idx]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop the unnecessary columns which are market risk premiums\n",
    "F_F_regression.drop(columns=['Date','Mkt-RF','SMB','HML','RMW','CMA','RF','excess_return'], inplace=True)\n",
    "F_F_regression = F_F_regression.dropna()\n",
    "F_F_regression.to_pickle('stock_universe.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "       ticker       date   adj_close  adj_volume    return  Mkt-RF_beta  \\\n100         A 2018-01-09   69.013902   2666711.0  0.024554     0.009027   \n101         A 2018-01-10   68.071536   2957184.0 -0.013655     0.008722   \n102         A 2018-01-11   68.081152   1511134.0  0.000141     0.007732   \n103         A 2018-01-12   68.975438   1448155.0  0.013136     0.007377   \n104         A 2018-01-16   68.494639   1703398.0 -0.006971     0.007580   \n...       ...        ...         ...         ...       ...          ...   \n706757    ZTS 2023-04-24  176.350000   1117741.0 -0.002996     0.008080   \n706758    ZTS 2023-04-25  173.020000   1151148.0 -0.018883     0.007815   \n706759    ZTS 2023-04-26  172.940000   1205198.0 -0.000462     0.007942   \n706760    ZTS 2023-04-27  173.950000   1682893.0  0.005840     0.008008   \n706761    ZTS 2023-04-28  175.780000   1606682.0  0.010520     0.007368   \n\n        SMB_beta  HML_beta  RMW_beta  CMA_beta  \n100     0.001063 -0.007793 -0.004662 -0.005455  \n101     0.000859 -0.006795 -0.006242 -0.005700  \n102     0.001027 -0.007521 -0.005757 -0.005344  \n103     0.000628 -0.007505 -0.005661 -0.005886  \n104     0.000491 -0.007457 -0.005694 -0.005725  \n...          ...       ...       ...       ...  \n706757  0.004188  0.000523 -0.001081 -0.001064  \n706758  0.004184  0.000368 -0.001543 -0.000980  \n706759  0.004660  0.000837 -0.001425 -0.001464  \n706760  0.004670  0.001035 -0.001673 -0.001201  \n706761  0.005284  0.001055 -0.002179 -0.001335  \n\n[647590 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ticker</th>\n      <th>date</th>\n      <th>adj_close</th>\n      <th>adj_volume</th>\n      <th>return</th>\n      <th>Mkt-RF_beta</th>\n      <th>SMB_beta</th>\n      <th>HML_beta</th>\n      <th>RMW_beta</th>\n      <th>CMA_beta</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>100</th>\n      <td>A</td>\n      <td>2018-01-09</td>\n      <td>69.013902</td>\n      <td>2666711.0</td>\n      <td>0.024554</td>\n      <td>0.009027</td>\n      <td>0.001063</td>\n      <td>-0.007793</td>\n      <td>-0.004662</td>\n      <td>-0.005455</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>A</td>\n      <td>2018-01-10</td>\n      <td>68.071536</td>\n      <td>2957184.0</td>\n      <td>-0.013655</td>\n      <td>0.008722</td>\n      <td>0.000859</td>\n      <td>-0.006795</td>\n      <td>-0.006242</td>\n      <td>-0.005700</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>A</td>\n      <td>2018-01-11</td>\n      <td>68.081152</td>\n      <td>1511134.0</td>\n      <td>0.000141</td>\n      <td>0.007732</td>\n      <td>0.001027</td>\n      <td>-0.007521</td>\n      <td>-0.005757</td>\n      <td>-0.005344</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>A</td>\n      <td>2018-01-12</td>\n      <td>68.975438</td>\n      <td>1448155.0</td>\n      <td>0.013136</td>\n      <td>0.007377</td>\n      <td>0.000628</td>\n      <td>-0.007505</td>\n      <td>-0.005661</td>\n      <td>-0.005886</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>A</td>\n      <td>2018-01-16</td>\n      <td>68.494639</td>\n      <td>1703398.0</td>\n      <td>-0.006971</td>\n      <td>0.007580</td>\n      <td>0.000491</td>\n      <td>-0.007457</td>\n      <td>-0.005694</td>\n      <td>-0.005725</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>706757</th>\n      <td>ZTS</td>\n      <td>2023-04-24</td>\n      <td>176.350000</td>\n      <td>1117741.0</td>\n      <td>-0.002996</td>\n      <td>0.008080</td>\n      <td>0.004188</td>\n      <td>0.000523</td>\n      <td>-0.001081</td>\n      <td>-0.001064</td>\n    </tr>\n    <tr>\n      <th>706758</th>\n      <td>ZTS</td>\n      <td>2023-04-25</td>\n      <td>173.020000</td>\n      <td>1151148.0</td>\n      <td>-0.018883</td>\n      <td>0.007815</td>\n      <td>0.004184</td>\n      <td>0.000368</td>\n      <td>-0.001543</td>\n      <td>-0.000980</td>\n    </tr>\n    <tr>\n      <th>706759</th>\n      <td>ZTS</td>\n      <td>2023-04-26</td>\n      <td>172.940000</td>\n      <td>1205198.0</td>\n      <td>-0.000462</td>\n      <td>0.007942</td>\n      <td>0.004660</td>\n      <td>0.000837</td>\n      <td>-0.001425</td>\n      <td>-0.001464</td>\n    </tr>\n    <tr>\n      <th>706760</th>\n      <td>ZTS</td>\n      <td>2023-04-27</td>\n      <td>173.950000</td>\n      <td>1682893.0</td>\n      <td>0.005840</td>\n      <td>0.008008</td>\n      <td>0.004670</td>\n      <td>0.001035</td>\n      <td>-0.001673</td>\n      <td>-0.001201</td>\n    </tr>\n    <tr>\n      <th>706761</th>\n      <td>ZTS</td>\n      <td>2023-04-28</td>\n      <td>175.780000</td>\n      <td>1606682.0</td>\n      <td>0.010520</td>\n      <td>0.007368</td>\n      <td>0.005284</td>\n      <td>0.001055</td>\n      <td>-0.002179</td>\n      <td>-0.001335</td>\n    </tr>\n  </tbody>\n</table>\n<p>647590 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universe_check = pd.read_pickle('stock_universe.pkl')\n",
    "universe_check"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Retrieving Headlines\n",
    "\n",
    "### 2.1 Set up\n",
    "\n",
    "You may need to install the libraries `beautifulsoup4` and `newspaper3k`.\n",
    "\n",
    "The `GNews` library needs to be installed  from the Github source. Here is a [StackOverflow forum] I referenced, in case it is helpful.\n",
    "\n",
    "After discussion, we plan to retrieve 5 equities for this project and they are: `AAPL`, `AMZN`, `FB`, `GOOG`, `MSFT`. Thus, the news data will only be retrieved for these 5 equities."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "1868ae19",
   "metadata": {},
   "source": [
    "### 2.2 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87418aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the 2 week periods\n",
    "\n",
    "first_day = np.ones(12, dtype = int)\n",
    "middle_day = np.repeat(15, 12)\n",
    "middle_day[1] = 14 # feb\n",
    "last_day = np.tile([31, 30], 6)\n",
    "last_day[7:12] = last_day[0:5]\n",
    "last_day[1] = 28 # feb\n",
    "\n",
    "start_days = []\n",
    "end_days = []\n",
    "\n",
    "for i in range(12):\n",
    "    \n",
    "    start_days.append(first_day[i])\n",
    "    end_days.append(middle_day[i])\n",
    "    \n",
    "    start_days.append(middle_day[i])\n",
    "    end_days.append(last_day[i])\n",
    "\n",
    "months = np.repeat(range(12), 2) + 1\n",
    "\n",
    "# print(start_days)\n",
    "# print(end_days)\n",
    "# print(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4535b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headlines(year, keyword):\n",
    "    \"\"\"\n",
    "    year: int \n",
    "    keyword: str, the company name\n",
    "    \"\"\"\n",
    "    \n",
    "    headlines_df = pd.DataFrame(columns = [\"date\", \"title\", \"publisher\"])\n",
    "    \n",
    "    for two_week_period in range(24):\n",
    "    \n",
    "        month = months[two_week_period]\n",
    "        start_day = start_days[two_week_period]\n",
    "        end_day = end_days[two_week_period]\n",
    "\n",
    "        start = dt.datetime(year, month, start_day)\n",
    "        end = dt.datetime(year, month, end_day)\n",
    "\n",
    "        gnews = GNews(language = \"en\",\n",
    "                      start_date = start, \n",
    "                      end_date = end)\n",
    "\n",
    "        news_df = pd.DataFrame(gnews.get_news(keyword))\n",
    "\n",
    "        if news_df.shape == (0, 0):\n",
    "            print(f\"No news between {start} and {end} for {keyword}.\\n\")\n",
    "            continue\n",
    "\n",
    "        news_df['date'] = pd.to_datetime(news_df['published date'])\n",
    "\n",
    "        headlines_df = pd.concat([headlines_df, news_df[['date', 'title', 'publisher']].copy()],\n",
    "                                 ignore_index = True)\n",
    "    \n",
    "    return headlines_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042c345",
   "metadata": {},
   "source": [
    "### 2.3 Retrieve data\n",
    "\n",
    "Only run one cell at a time!\n",
    "\n",
    "When running these cells, you will get errors about having no news for certain time periods. That's fine, don't re-run the cell. Keep it the way it is so we have records about when the headlines were missing. Just commit and push what you have from that one run.\n",
    "\n",
    "One day later, it can be helpful to duplicate the cell, change the `range(2018, 2023+1)` to start from whichever year there is missing headlines, and run the code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wafer: apple, amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591206eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apple\n",
    "\n",
    "company = \"Apple\"\n",
    "\n",
    "for year in range(2018, 2023+1):\n",
    "\n",
    "    headlines_df = get_headlines(year, company)\n",
    "\n",
    "    file = \"headlines/\" + str(year) + \"_\" + company + \"_headlines.csv\"\n",
    "    headlines_df.to_csv(file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3559c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amazon\n",
    "\n",
    "company = \"Amazon\"\n",
    "\n",
    "for year in range(2018, 2023+1):\n",
    "\n",
    "    headlines_df = get_headlines(year, company)\n",
    "\n",
    "    file = \"headlines/\" + str(year) + \"_\" + company + \"_headlines.csv\"\n",
    "    headlines_df.to_csv(file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b4ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cindy: nvidia, microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4770bcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No news between 2023-07-15 00:00:00 and 2023-07-31 00:00:00 for Google.\n",
      "\n",
      "No news between 2023-08-01 00:00:00 and 2023-08-15 00:00:00 for Google.\n",
      "\n",
      "No news between 2023-08-15 00:00:00 and 2023-08-31 00:00:00 for Google.\n",
      "\n",
      "No news between 2023-09-01 00:00:00 and 2023-09-15 00:00:00 for Google.\n",
      "\n",
      "No news between 2023-09-15 00:00:00 and 2023-09-30 00:00:00 for Google.\n",
      "\n",
      "No news between 2023-10-01 00:00:00 and 2023-10-15 00:00:00 for Google.\n",
      "\n",
      "No news between 2023-10-15 00:00:00 and 2023-10-31 00:00:00 for Google.\n",
      "\n",
      "No news between 2023-11-01 00:00:00 and 2023-11-15 00:00:00 for Google.\n",
      "\n",
      "No news between 2023-11-15 00:00:00 and 2023-11-30 00:00:00 for Google.\n",
      "\n",
      "No news between 2023-12-01 00:00:00 and 2023-12-15 00:00:00 for Google.\n",
      "\n",
      "No news between 2023-12-15 00:00:00 and 2023-12-31 00:00:00 for Google.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nvidia\n",
    "\n",
    "company = \"Google\"\n",
    "\n",
    "for year in range(2018, 2023+1):\n",
    "\n",
    "    headlines_df = get_headlines(year, company)\n",
    "\n",
    "    file = \"headlines/\" + str(year) + \"_\" + company + \"_headlines.csv\"\n",
    "    headlines_df.to_csv(file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c690786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No news between 2023-07-15 00:00:00 and 2023-07-31 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-08-01 00:00:00 and 2023-08-15 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-08-15 00:00:00 and 2023-08-31 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-09-01 00:00:00 and 2023-09-15 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-09-15 00:00:00 and 2023-09-30 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-10-01 00:00:00 and 2023-10-15 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-10-15 00:00:00 and 2023-10-31 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-11-01 00:00:00 and 2023-11-15 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-11-15 00:00:00 and 2023-11-30 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-12-01 00:00:00 and 2023-12-15 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-12-15 00:00:00 and 2023-12-31 00:00:00 for Microsoft.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# microsoft\n",
    "\n",
    "company = \"Microsoft\"\n",
    "\n",
    "for year in range(2018, 2023+1):\n",
    "\n",
    "    headlines_df = get_headlines(year, company)\n",
    "\n",
    "    file = \"headlines/\" + str(year) + \"_\" + company + \"_headlines.csv\"\n",
    "    headlines_df.to_csv(file, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Combine data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "63d02f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "AAPL = pd.DataFrame()\n",
    "MSFT = pd.DataFrame()\n",
    "AMZN = pd.DataFrame()\n",
    "GOOG = pd.DataFrame()\n",
    "NVDA = pd.DataFrame()\n",
    "for year in range(2018, 2023+1):\n",
    "    AMZN = AMZN.append([pd.read_csv('headlines/' + str(year) + '_Amazon_headlines.csv')],\n",
    "              ignore_index = True)\n",
    "    AAPL = AAPL.append([pd.read_csv('headlines/' + str(year) + '_Apple_headlines.csv')],\n",
    "              ignore_index = True)\n",
    "    GOOG = GOOG.append([pd.read_csv('headlines/' + str(year) + '_Google_headlines.csv')],\n",
    "              ignore_index = True)\n",
    "    MSFT = MSFT.append([pd.read_csv('headlines/' + str(year) + '_Microsoft_headlines.csv')],\n",
    "              ignore_index = True)\n",
    "    NVDA = NVDA.append([pd.read_csv('headlines/' + str(year) + '_Nvidia_headlines.csv')],\n",
    "              ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7cdac5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.insert(1,'ticker', 'AMZN')\n",
    "AAPL.insert(1, 'ticker', 'AAPL')\n",
    "GOOG.insert(1, 'ticker', 'GOOG')\n",
    "MSFT.insert(1, 'ticker', 'MSFT')\n",
    "NVDA.insert(1, 'ticker', 'NVDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7079c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = pd.concat([AMZN, AAPL, GOOG, MSFT, NVDA], ignore_index = True)\n",
    "headlines['date'] = pd.to_datetime(headlines['date']).dt.tz_convert(None)\n",
    "headlines.sort_values(by = ['date','ticker'], ignore_index = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fd4fdc73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>title</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 08:00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3 tips to maximize Apple's free Pages word pro...</td>\n",
       "      <td>TechRepublic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 08:00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>The 20 best iOS games of 2017 - Macworld</td>\n",
       "      <td>Macworld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 08:00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>How to Switch Apple Watch Home Screen from Gri...</td>\n",
       "      <td>Wccftech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 08:00:00</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Will Ferrell, Molly Shannon Tease Trump and Ti...</td>\n",
       "      <td>Hollywood Reporter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 08:00:00</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>The Limits of Amazon - WSJ - The Wall Street J...</td>\n",
       "      <td>The Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65704</th>\n",
       "      <td>2023-07-11 19:49:00</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>Google's Searchbot Could Put Me Out of a Job -...</td>\n",
       "      <td>The Atlantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65705</th>\n",
       "      <td>2023-07-11 20:08:55</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>Google On Fixing Discovered Currently Not Inde...</td>\n",
       "      <td>Search Engine Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65706</th>\n",
       "      <td>2023-07-11 20:16:08</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>Google’s head of AR software quits, citing “un...</td>\n",
       "      <td>Ars Technica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65707</th>\n",
       "      <td>2023-07-11 23:30:19</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>Google quietly ditched plans for an AI-powered...</td>\n",
       "      <td>CNBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65708</th>\n",
       "      <td>2023-07-12 01:09:00</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>Google hit with class-action lawsuit over AI d...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65709 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date ticker  \\\n",
       "0     2018-01-01 08:00:00   AAPL   \n",
       "1     2018-01-01 08:00:00   AAPL   \n",
       "2     2018-01-01 08:00:00   AAPL   \n",
       "3     2018-01-01 08:00:00   AMZN   \n",
       "4     2018-01-01 08:00:00   AMZN   \n",
       "...                   ...    ...   \n",
       "65704 2023-07-11 19:49:00   GOOG   \n",
       "65705 2023-07-11 20:08:55   GOOG   \n",
       "65706 2023-07-11 20:16:08   GOOG   \n",
       "65707 2023-07-11 23:30:19   GOOG   \n",
       "65708 2023-07-12 01:09:00   GOOG   \n",
       "\n",
       "                                                   title  \\\n",
       "0      3 tips to maximize Apple's free Pages word pro...   \n",
       "1               The 20 best iOS games of 2017 - Macworld   \n",
       "2      How to Switch Apple Watch Home Screen from Gri...   \n",
       "3      Will Ferrell, Molly Shannon Tease Trump and Ti...   \n",
       "4      The Limits of Amazon - WSJ - The Wall Street J...   \n",
       "...                                                  ...   \n",
       "65704  Google's Searchbot Could Put Me Out of a Job -...   \n",
       "65705  Google On Fixing Discovered Currently Not Inde...   \n",
       "65706  Google’s head of AR software quits, citing “un...   \n",
       "65707  Google quietly ditched plans for an AI-powered...   \n",
       "65708  Google hit with class-action lawsuit over AI d...   \n",
       "\n",
       "                     publisher  \n",
       "0                 TechRepublic  \n",
       "1                     Macworld  \n",
       "2                     Wccftech  \n",
       "3           Hollywood Reporter  \n",
       "4      The Wall Street Journal  \n",
       "...                        ...  \n",
       "65704             The Atlantic  \n",
       "65705    Search Engine Journal  \n",
       "65706             Ars Technica  \n",
       "65707                     CNBC  \n",
       "65708                  Reuters  \n",
       "\n",
       "[65709 rows x 4 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines['publisher'] = headlines['publisher'].apply(lambda x: ' '.join(x.split()[3:]).strip(\"'\").strip(\"}\")[:-1])\n",
    "headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9b8f8d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines.to_csv('headlines.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df37443",
   "metadata": {},
   "source": [
    "## 3. Add Sentiment Analysis to Headlines Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1b3b268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>title</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 08:00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3 tips to maximize Apple's free Pages word pro...</td>\n",
       "      <td>TechRepublic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 08:00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>The 20 best iOS games of 2017 - Macworld</td>\n",
       "      <td>Macworld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 08:00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>How to Switch Apple Watch Home Screen from Gri...</td>\n",
       "      <td>Wccftech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 08:00:00</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Will Ferrell, Molly Shannon Tease Trump and Ti...</td>\n",
       "      <td>Hollywood Reporter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 08:00:00</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>The Limits of Amazon - WSJ - The Wall Street J...</td>\n",
       "      <td>The Wall Street Journal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date ticker  \\\n",
       "0  2018-01-01 08:00:00   AAPL   \n",
       "1  2018-01-01 08:00:00   AAPL   \n",
       "2  2018-01-01 08:00:00   AAPL   \n",
       "3  2018-01-01 08:00:00   AMZN   \n",
       "4  2018-01-01 08:00:00   AMZN   \n",
       "\n",
       "                                               title                publisher  \n",
       "0  3 tips to maximize Apple's free Pages word pro...             TechRepublic  \n",
       "1           The 20 best iOS games of 2017 - Macworld                 Macworld  \n",
       "2  How to Switch Apple Watch Home Screen from Gri...                 Wccftech  \n",
       "3  Will Ferrell, Molly Shannon Tease Trump and Ti...       Hollywood Reporter  \n",
       "4  The Limits of Amazon - WSJ - The Wall Street J...  The Wall Street Journal  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines = pd.read_csv('headlines.csv')\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a2907",
   "metadata": {},
   "source": [
    "### 3.1 Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185d738",
   "metadata": {},
   "source": [
    "**Remove publisher tag -- this a rough solution that works for most tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9353117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_source_tag(headline):\n",
    "    \"\"\"\n",
    "    after each headline, there is a \" - [source name]\" like \" - New York Times\". \n",
    "    \n",
    "    this function removes the final occurence of \" - ...\"\n",
    "    it doesn't perfectly remove the source tags, ie \" - WSJ - The Wall Street Journal\" becomes \"WSJ\"\n",
    "    \n",
    "    headline: string\n",
    "    \"\"\"\n",
    "    \n",
    "    split_title = headline.split(' - ')\n",
    "    n_splits = len(split_title) - 1\n",
    "    \n",
    "    return(' '.join(split_title[0:n_splits]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67cf02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_headlines(df, start_date='2018-01-01', end_date='2023-07-01'):\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['date'] = df['date'].apply(lambda ts: ts.replace(hour=0, minute=0, second=0))\n",
    "\n",
    "    # date range\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = pd.to_datetime(end_date)\n",
    "    \n",
    "    df = df.loc[(df['date'] >= start) & (df['date'] <= end)]\n",
    "    \n",
    "    # contains company names\n",
    "    df = df.loc[pd.Series(\n",
    "        df['title']\n",
    "    ).str.contains(\"|\".join(['Apple', 'Amazon', 'Google', \n",
    "                             'Microsoft','Nvidia']), case = False)]\n",
    "    \n",
    "    # remove duplicates\n",
    "    df.drop_duplicates()\n",
    "    \n",
    "    df['title'] = df['title'].apply(lambda headline : remove_source_tag(headline))\n",
    "    \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4728cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = clean_headlines(headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b73ba",
   "metadata": {},
   "source": [
    "### 3.2 Add scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4102dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_scores(df):\n",
    "    \n",
    "    compound = []\n",
    "    neg = []\n",
    "    neu = []\n",
    "    pos = []\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    for headline in df['title']:\n",
    "        ss = sid.polarity_scores(headline)\n",
    "\n",
    "        compound.append(ss['compound'])\n",
    "        neg.append(ss['neg'])\n",
    "        neu.append(ss['neu'])\n",
    "        pos.append(ss['pos'])\n",
    "        \n",
    "    df['compound'] = compound\n",
    "    df['neg'] = neg\n",
    "    df['neu'] = neu\n",
    "    df['pos'] = pos\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a9c8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_day(df):\n",
    "    \n",
    "    \n",
    "    # number of headlines per day\n",
    "    volume = df.groupby(['date', 'ticker']).count().iloc[:, 1]\n",
    "    \n",
    "    # take avg\n",
    "    df = df.groupby(['date', 'ticker']).mean(numeric_only = True)\n",
    "    df['volume'] = volume\n",
    "    \n",
    "    return df.reset_index().sort_values('date')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce22ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = add_sentiment_scores(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70cdd096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>title</th>\n",
       "      <th>publisher</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3 tips to maximize Apple's free Pages word pro...</td>\n",
       "      <td>TechRepublic</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>How to Switch Apple Watch Home Screen from Gri...</td>\n",
       "      <td>Wccftech</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Will Ferrell, Molly Shannon Tease Trump and Ti...</td>\n",
       "      <td>Hollywood Reporter</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>The Limits of Amazon WSJ</td>\n",
       "      <td>The Wall Street Journal</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon Mistakenly Sends AWS Budget Emails Fore...</td>\n",
       "      <td>BleepingComputer</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56959</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>New Apple Watch Ultra, 30-inch iMac make us sa...</td>\n",
       "      <td>Cult of Mac</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56960</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>A quick look back at Microsoft Bob, which was ...</td>\n",
       "      <td>Neowin</td>\n",
       "      <td>-0.6249</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56961</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>The Best Apple AirPods Models to Buy in 2023</td>\n",
       "      <td>IGN</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56962</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>How Google Beat Apple To AR And Still Failed</td>\n",
       "      <td>SlashGear</td>\n",
       "      <td>-0.5106</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56963</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>Nvidia Dlss 3 Plug-In Available Now For Unreal...</td>\n",
       "      <td>MENAFN.COM</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56964 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date ticker                                              title  \\\n",
       "0     2018-01-01   AAPL  3 tips to maximize Apple's free Pages word pro...   \n",
       "1     2018-01-01   AAPL  How to Switch Apple Watch Home Screen from Gri...   \n",
       "2     2018-01-01   AMZN  Will Ferrell, Molly Shannon Tease Trump and Ti...   \n",
       "3     2018-01-01   AMZN                           The Limits of Amazon WSJ   \n",
       "4     2018-01-01   AMZN  Amazon Mistakenly Sends AWS Budget Emails Fore...   \n",
       "...          ...    ...                                                ...   \n",
       "56959 2023-07-01   AAPL  New Apple Watch Ultra, 30-inch iMac make us sa...   \n",
       "56960 2023-07-01   MSFT  A quick look back at Microsoft Bob, which was ...   \n",
       "56961 2023-07-01   AAPL       The Best Apple AirPods Models to Buy in 2023   \n",
       "56962 2023-07-01   AAPL       How Google Beat Apple To AR And Still Failed   \n",
       "56963 2023-07-01   NVDA  Nvidia Dlss 3 Plug-In Available Now For Unreal...   \n",
       "\n",
       "                     publisher  compound    neg    neu    pos  \n",
       "0                 TechRepublic    0.5106  0.000  0.708  0.292  \n",
       "1                     Wccftech    0.0000  0.000  1.000  0.000  \n",
       "2           Hollywood Reporter   -0.3182  0.141  0.859  0.000  \n",
       "3      The Wall Street Journal    0.1779  0.000  0.702  0.298  \n",
       "4             BleepingComputer    0.4215  0.167  0.455  0.379  \n",
       "...                        ...       ...    ...    ...    ...  \n",
       "56959              Cult of Mac    0.0000  0.000  1.000  0.000  \n",
       "56960                   Neowin   -0.6249  0.215  0.785  0.000  \n",
       "56961                      IGN    0.6369  0.000  0.656  0.344  \n",
       "56962                SlashGear   -0.5106  0.292  0.708  0.000  \n",
       "56963               MENAFN.COM    0.0000  0.000  1.000  0.000  \n",
       "\n",
       "[56964 rows x 8 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "384c82a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.255300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>0.169250</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>0.249680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>0.484700</td>\n",
       "      <td>0.081500</td>\n",
       "      <td>0.594500</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.064093</td>\n",
       "      <td>0.093143</td>\n",
       "      <td>0.815571</td>\n",
       "      <td>0.091286</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9263</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>-0.061820</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.888800</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9264</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>-0.129689</td>\n",
       "      <td>0.067333</td>\n",
       "      <td>0.932667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9261</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.112262</td>\n",
       "      <td>0.022462</td>\n",
       "      <td>0.897308</td>\n",
       "      <td>0.080231</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9262</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.253340</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.700600</td>\n",
       "      <td>0.258900</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9265</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>0.158071</td>\n",
       "      <td>0.031571</td>\n",
       "      <td>0.860429</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9266 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date ticker  compound       neg       neu       pos  volume\n",
       "0    2018-01-01   AAPL  0.255300  0.000000  0.854000  0.146000       2\n",
       "1    2018-01-01   AMZN  0.070300  0.077000  0.754000  0.169250       4\n",
       "2    2018-01-01   GOOG  0.249680  0.000000  0.843000  0.157000       5\n",
       "3    2018-01-01   MSFT  0.484700  0.081500  0.594500  0.324000       2\n",
       "4    2018-01-02   AAPL  0.064093  0.093143  0.815571  0.091286      14\n",
       "...         ...    ...       ...       ...       ...       ...     ...\n",
       "9263 2023-07-01   GOOG -0.061820  0.071200  0.888800  0.040000       5\n",
       "9264 2023-07-01   MSFT -0.129689  0.067333  0.932667  0.000000       9\n",
       "9261 2023-07-01   AAPL  0.112262  0.022462  0.897308  0.080231      13\n",
       "9262 2023-07-01   AMZN  0.253340  0.040400  0.700600  0.258900      10\n",
       "9265 2023-07-01   NVDA  0.158071  0.031571  0.860429  0.108000       7\n",
       "\n",
       "[9266 rows x 7 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_headlines = wrangle_day(headlines)\n",
    "day_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "48bbac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_headlines.to_csv('daily_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b73fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
