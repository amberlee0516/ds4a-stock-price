{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8298734",
   "metadata": {},
   "source": [
    "# Retrieving headlines\n",
    "\n",
    "### Set up\n",
    "\n",
    "You may need to install the libraries `beautifulsoup4` and `newspaper3k`.\n",
    "\n",
    "The `GNews` library needs to be installed  from the Github source. Here is a [StackOverflow forum] I referenced, in case it is helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe2b5ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.9/site-packages (4.11.1)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.3.1)\r\n",
      "Collecting newspaper3k\r\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m211.1/211.1 kB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (4.11.1)\r\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (9.4.0)\r\n",
      "Requirement already satisfied: PyYAML>=3.11 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (6.0)\r\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (1.1.0)\r\n",
      "Requirement already satisfied: lxml>=3.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (4.9.1)\r\n",
      "Requirement already satisfied: nltk>=3.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (3.7)\r\n",
      "Requirement already satisfied: requests>=2.10.0 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (2.28.1)\r\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\r\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m81.1/81.1 kB\u001B[0m \u001B[31m1.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: tldextract>=2.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (3.2.0)\r\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\r\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting jieba3k>=0.35.1 (from newspaper3k)\r\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.4/7.4 MB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/lib/python3.9/site-packages (from newspaper3k) (2.8.2)\r\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\r\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.3.1)\r\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.9/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\r\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\r\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: click in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.2.1->newspaper3k) (8.0.4)\r\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.2.1->newspaper3k) (1.1.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.2.1->newspaper3k) (2022.7.9)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.2.1->newspaper3k) (4.64.1)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->newspaper3k) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->newspaper3k) (1.26.11)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->newspaper3k) (2022.9.24)\r\n",
      "Requirement already satisfied: requests-file>=1.4 in /opt/anaconda3/lib/python3.9/site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\r\n",
      "Requirement already satisfied: filelock>=3.0.8 in /opt/anaconda3/lib/python3.9/site-packages (from tldextract>=2.0.1->newspaper3k) (3.6.0)\r\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\r\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13541 sha256=c0edc9ad0364606115b836e13f899baa201d1ab82a8fcf995bbad4c67631f720\r\n",
      "  Stored in directory: /Users/cindy/Library/Caches/pip/wheels/94/ad/df/a2a01300cea47d5695f242f7e925a805970106fd9e4b151468\r\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=cf03bf4817bc84e648544148198e2ef4dc7a066d6021fb7db3451df0e4ba431b\r\n",
      "  Stored in directory: /Users/cindy/Library/Caches/pip/wheels/43/4a/c2/61a371b2524ac90805391c660d8dc4505705297f25e2b85a5d\r\n",
      "  Building wheel for jieba3k (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398382 sha256=36692c41d7199eebf6c708d9c75195236769cd38fdd6a47bae402f916f07b213\r\n",
      "  Stored in directory: /Users/cindy/Library/Caches/pip/wheels/c2/22/59/8214a8d6357e9f540ce1f37f9a4362b6156b4ca81b37f1945f\r\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=20e165eb18b63f1f8ac946911ee1ebf85e6484ecdb333ec81c29c388c48c1d2c\r\n",
      "  Stored in directory: /Users/cindy/Library/Caches/pip/wheels/65/7a/a7/78c287f64e401255dff4c13fdbc672fed5efbfd21c530114e1\r\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\r\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, feedfinder2, newspaper3k\r\n",
      "Successfully installed feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 sgmllib3k-1.0.0 tinysegmenter-0.3\r\n",
      "Collecting git+https://github.com/ranahaani/GNews.git\r\n",
      "  Cloning https://github.com/ranahaani/GNews.git to /private/var/folders/97/ry8p7x356bbbk7qdw9k14pyr0000gn/T/pip-req-build-l10ne1ck\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ranahaani/GNews.git /private/var/folders/97/ry8p7x356bbbk7qdw9k14pyr0000gn/T/pip-req-build-l10ne1ck\r\n",
      "  Resolved https://github.com/ranahaani/GNews.git to commit 8591313e3fdaaf44e2e09f2265254fc3aaea8b56\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: feedparser~=6.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from gnews==0.2.7) (6.0.10)\r\n",
      "Collecting bs4~=0.0.1 (from gnews==0.2.7)\r\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting beautifulsoup4~=4.9.3 (from gnews==0.2.7)\r\n",
      "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m115.8/115.8 kB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting pymongo~=3.12.0 (from gnews==0.2.7)\r\n",
      "  Downloading pymongo-3.12.3-cp39-cp39-macosx_10_9_x86_64.whl (395 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m395.4/395.4 kB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting dnspython~=1.16.0 (from gnews==0.2.7)\r\n",
      "  Downloading dnspython-1.16.0-py2.py3-none-any.whl (188 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m188.4/188.4 kB\u001B[0m \u001B[31m603.4 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting python-dotenv~=0.19.0 (from gnews==0.2.7)\r\n",
      "  Downloading python_dotenv-0.19.2-py2.py3-none-any.whl (17 kB)\r\n",
      "Collecting requests==2.26.0 (from gnews==0.2.7)\r\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.3/62.3 kB\u001B[0m \u001B[31m1.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests==2.26.0->gnews==0.2.7) (1.26.11)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests==2.26.0->gnews==0.2.7) (2022.9.24)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests==2.26.0->gnews==0.2.7) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests==2.26.0->gnews==0.2.7) (3.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4~=4.9.3->gnews==0.2.7) (2.3.1)\r\n",
      "Requirement already satisfied: sgmllib3k in /opt/anaconda3/lib/python3.9/site-packages (from feedparser~=6.0.2->gnews==0.2.7) (1.0.0)\r\n",
      "Building wheels for collected packages: gnews, bs4\r\n",
      "  Building wheel for gnews (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for gnews: filename=gnews-0.2.7-py3-none-any.whl size=16161 sha256=c96a5b0cc03d40a49b997e2361ce877b195278d5b611c5958355f2cbd8c5ae9e\r\n",
      "  Stored in directory: /private/var/folders/97/ry8p7x356bbbk7qdw9k14pyr0000gn/T/pip-ephem-wheel-cache-sfptsfot/wheels/17/57/85/1c4d5a4488fc09b600274e90a6a5ea4f8f63d672688a9ac885\r\n",
      "  Building wheel for bs4 (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=61f568861651c23b3556c58ace7344dd3e23f08f644c1d017511af9423833ff4\r\n",
      "  Stored in directory: /Users/cindy/Library/Caches/pip/wheels/73/2b/cb/099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\r\n",
      "Successfully built gnews bs4\r\n",
      "Installing collected packages: requests, python-dotenv, pymongo, dnspython, beautifulsoup4, bs4, gnews\r\n",
      "  Attempting uninstall: requests\r\n",
      "    Found existing installation: requests 2.28.1\r\n",
      "    Uninstalling requests-2.28.1:\r\n",
      "      Successfully uninstalled requests-2.28.1\r\n",
      "  Attempting uninstall: python-dotenv\r\n",
      "    Found existing installation: python-dotenv 1.0.0\r\n",
      "    Uninstalling python-dotenv-1.0.0:\r\n",
      "      Successfully uninstalled python-dotenv-1.0.0\r\n",
      "  Attempting uninstall: beautifulsoup4\r\n",
      "    Found existing installation: beautifulsoup4 4.11.1\r\n",
      "    Uninstalling beautifulsoup4-4.11.1:\r\n",
      "      Successfully uninstalled beautifulsoup4-4.11.1\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\r\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\r\n",
      "conda-repo-cli 1.0.20 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\r\n",
      "conda-repo-cli 1.0.20 requires nbformat==5.4.0, but you have nbformat 5.5.0 which is incompatible.\r\n",
      "conda-repo-cli 1.0.20 requires requests==2.28.1, but you have requests 2.26.0 which is incompatible.\r\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 2.0.1 which is incompatible.\r\n",
      "yfinance 0.2.12 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.9.3 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed beautifulsoup4-4.9.3 bs4-0.0.1 dnspython-1.16.0 gnews-0.2.7 pymongo-3.12.3 python-dotenv-0.19.2 requests-2.26.0\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install beautifulsoup4\n",
    "!{sys.executable} -m pip install newspaper3k\n",
    "!{sys.executable} -m pip install git+https://github.com/ranahaani/GNews.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cd78bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/10/2023 09:31:15 PM - Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "07/10/2023 09:31:15 PM - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "from gnews import GNews\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868ae19",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87418aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the 2 week periods\n",
    "\n",
    "first_day = np.ones(12, dtype = int)\n",
    "middle_day = np.repeat(15, 12)\n",
    "middle_day[1] = 14 # feb\n",
    "last_day = np.tile([31, 30], 6)\n",
    "last_day[7:12] = last_day[0:5]\n",
    "last_day[1] = 28 # feb\n",
    "\n",
    "start_days = []\n",
    "end_days = []\n",
    "\n",
    "for i in range(12):\n",
    "    \n",
    "    start_days.append(first_day[i])\n",
    "    end_days.append(middle_day[i])\n",
    "    \n",
    "    start_days.append(middle_day[i])\n",
    "    end_days.append(last_day[i])\n",
    "\n",
    "months = np.repeat(range(12), 2) + 1\n",
    "\n",
    "# print(start_days)\n",
    "# print(end_days)\n",
    "# print(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4535b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headlines(year, keyword):\n",
    "    \"\"\"\n",
    "    year: int \n",
    "    keyword: str, the company name\n",
    "    \"\"\"\n",
    "    \n",
    "    headlines_df = pd.DataFrame(columns = [\"date\", \"title\", \"publisher\"])\n",
    "    \n",
    "    for two_week_period in range(24):\n",
    "    \n",
    "        month = months[two_week_period]\n",
    "        start_day = start_days[two_week_period]\n",
    "        end_day = end_days[two_week_period]\n",
    "\n",
    "        start = dt.datetime(year, month, start_day)\n",
    "        end = dt.datetime(year, month, end_day)\n",
    "\n",
    "        gnews = GNews(language = \"en\",\n",
    "                      start_date = start, \n",
    "                      end_date = end)\n",
    "\n",
    "        news_df = pd.DataFrame(gnews.get_news(keyword))\n",
    "\n",
    "        if news_df.shape == (0, 0):\n",
    "            print(f\"No news between {start} and {end} for {keyword}.\\n\")\n",
    "            continue\n",
    "\n",
    "        news_df['date'] = pd.to_datetime(news_df['published date'])\n",
    "\n",
    "        headlines_df = pd.concat([headlines_df, news_df[['date', 'title', 'publisher']].copy()],\n",
    "                                 ignore_index = True)\n",
    "    \n",
    "    return headlines_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042c345",
   "metadata": {},
   "source": [
    "### Retrieve data\n",
    "\n",
    "Only run one cell at a time!\n",
    "\n",
    "When running these cells, you will get errors about having no news for certain time periods. That's fine, don't re-run the cell. Keep it the way it is so we have records about when the headlines were missing. Just commit and push what you have from that one run.\n",
    "\n",
    "One day later, it can be helpful to duplicate the cell, change the `range(2018, 2023+1)` to start from whichever year there is missing headlines, and run the code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wafer: apple, amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591206eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apple\n",
    "\n",
    "company = \"Apple\"\n",
    "\n",
    "for year in range(2018, 2023+1):\n",
    "\n",
    "    headlines_df = get_headlines(year, company)\n",
    "\n",
    "    file = \"headlines/\" + str(year) + \"_\" + company + \"_headlines.csv\"\n",
    "    headlines_df.to_csv(file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3559c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amazon\n",
    "\n",
    "company = \"Amazon\"\n",
    "\n",
    "for year in range(2018, 2023+1):\n",
    "\n",
    "    headlines_df = get_headlines(year, company)\n",
    "\n",
    "    file = \"headlines/\" + str(year) + \"_\" + company + \"_headlines.csv\"\n",
    "    headlines_df.to_csv(file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b4ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cindy: nvidia, microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4770bcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No news between 2023-07-15 00:00:00 and 2023-07-31 00:00:00 for Nvidia.\n",
      "\n",
      "No news between 2023-08-01 00:00:00 and 2023-08-15 00:00:00 for Nvidia.\n",
      "\n",
      "No news between 2023-08-15 00:00:00 and 2023-08-31 00:00:00 for Nvidia.\n",
      "\n",
      "No news between 2023-09-01 00:00:00 and 2023-09-15 00:00:00 for Nvidia.\n",
      "\n",
      "No news between 2023-09-15 00:00:00 and 2023-09-30 00:00:00 for Nvidia.\n",
      "\n",
      "No news between 2023-10-01 00:00:00 and 2023-10-15 00:00:00 for Nvidia.\n",
      "\n",
      "No news between 2023-10-15 00:00:00 and 2023-10-31 00:00:00 for Nvidia.\n",
      "\n",
      "No news between 2023-11-01 00:00:00 and 2023-11-15 00:00:00 for Nvidia.\n",
      "\n",
      "No news between 2023-11-15 00:00:00 and 2023-11-30 00:00:00 for Nvidia.\n",
      "\n",
      "No news between 2023-12-01 00:00:00 and 2023-12-15 00:00:00 for Nvidia.\n",
      "\n",
      "No news between 2023-12-15 00:00:00 and 2023-12-31 00:00:00 for Nvidia.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nvidia\n",
    "\n",
    "company = \"Nvidia\"\n",
    "\n",
    "for year in range(2018, 2023+1):\n",
    "\n",
    "    headlines_df = get_headlines(year, company)\n",
    "\n",
    "    file = \"headlines/\" + str(year) + \"_\" + company + \"_headlines.csv\"\n",
    "    headlines_df.to_csv(file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c690786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No news between 2023-07-15 00:00:00 and 2023-07-31 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-08-01 00:00:00 and 2023-08-15 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-08-15 00:00:00 and 2023-08-31 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-09-01 00:00:00 and 2023-09-15 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-09-15 00:00:00 and 2023-09-30 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-10-01 00:00:00 and 2023-10-15 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-10-15 00:00:00 and 2023-10-31 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-11-01 00:00:00 and 2023-11-15 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-11-15 00:00:00 and 2023-11-30 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-12-01 00:00:00 and 2023-12-15 00:00:00 for Microsoft.\n",
      "\n",
      "No news between 2023-12-15 00:00:00 and 2023-12-31 00:00:00 for Microsoft.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# microsoft\n",
    "\n",
    "company = \"Microsoft\"\n",
    "\n",
    "for year in range(2018, 2023+1):\n",
    "\n",
    "    headlines_df = get_headlines(year, company)\n",
    "\n",
    "    file = \"headlines/\" + str(year) + \"_\" + company + \"_headlines.csv\"\n",
    "    headlines_df.to_csv(file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
